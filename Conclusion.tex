\chapter{Conclusion}

\yinipar{\fontsize{60pt}{72pt}\usefont{U}{Kramer}{xl}{n}W}e have come to the end of our journey. I hope this note lived up to its promises, and that the reader now understands better how a neural network is designed and how it works under the hood. To wrap it up, we have seen the architecture of the three most common neural networks, as well as the careful mathematical derivation of their training formulas.

\vspace{0.2cm}

Deep Learning seems to be a fast evolving field, and this material might be out of date in a near future, but the index approach adopted will still allow the reader -- as it as helped the writer -- to work out for herself what is behind the next state of the art architectures.

\vspace{0.2cm}

Until then, one should have enough material to encode from scratch its own FNN, CNN and RNN-LSTM, as the author did as an empirical proof of his formulas.
